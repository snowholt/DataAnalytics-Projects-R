---
title: "Case Study2: Bellabeat"
author: "Nariman Jafarieshlaghi"
date: "2023-03-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>
<br>

## How Can a Wellness Technology Company Play It Smart?  
  
<br>
<br>
### Introduction

* In this project, I analyzed smart device fitness data to gain insights into how consumers use their smart devices. The goal was to help guide Bellabeat's marketing strategy and unlock new growth opportunities for the company. Through the data analysis process, I identified key patterns and trends in the data and used these insights to make high-level recommendations for Bellabeat's marketing strategy. This project demonstrates my skills in data analysis and provides tangible evidence of my ability to deliver actionable insights to drive business growth.
<br>

### 1. Ask 
<br>

#### Guiding questions 

* What is the problem you are trying to solve? 
  * The problem we are trying to solve is to gain insights into how consumers use non-Bellabeat smart devices, and to apply those insights to a Bellabeat product in order to inform the company's  marketing strategy.

* How can your insights drive business decisions?
  * By analyzing smart device usage data, we can identify patterns and insights that can inform our marketing strategy for a specific Bellabeat product. These insights can help us to better understand our target audience, their behaviors, and preferences, and adjust our marketing efforts accordingly. For example, we may discover that a certain demographic uses smart devices in a particular way, and we can then tailor our messaging and advertising to appeal to that group. This can help us to improve the effectiveness of our marketing efforts and increase sales of our products.

<br>

#### Key tasks 

* Identify the business task.
  * The business task is to gain insights into how consumers use non-Bellabeat smart devices, and to apply those insights to a Bellabeat product in order to inform the company's marketing strategy. Specifically, the goal is to understand the smart device usage patterns and behaviors of our target audience, and use that information to better position and market our product in a way that resonates with our customers and increases sales.
    
* Consider key stakeholders
  * In this case, the key stakeholders could include:

    * Urška Sršen, the founder of Bellabeat
    * The marketing analytics team responsible for analyzing the data and providing insights
    * The product development team responsible for designing and improving the Bellabeat product being analyzed
    * The sales team responsible for selling the product and achieving revenue targets
    * The customers who use the product and are impacted by the marketing decisions made based on the insights from the data analysis

<br>

#### Deliverable 

A clear statement of the business task


<br>
<br>
<br>

### 2. Prepare

<br>
<br>

#### Guiding questions 
  
* Where is your data stored? 
  * Data is stored locally on a hard rive.

* How is the data organized? Is it in long or wide format? 
  * Data contains 18 files of daily, hourly, and minute logs. Most files are organized in a wide format. However, there are a few files that are organized in a long format. 

* Are there issues with bias or credibility in this data? Does your data ROCCC?
  * Bias:
    * Sampling bias: Since the data was generated by respondents to a distributed survey via Amazon Mechanical Turk, there may be a risk of selection bias, as the sample may not be representative of the general population. The sample may be biased towards people who are more likely to use Amazon Mechanical Turk or use Fitbit devices, which could skew the results.
    * Self-selection bias: Since the data was generated by Fitbit users who consented to submit their personal tracker data, there may be a risk of self-selection bias, as the sample may not be representative of all Fitbit users. People who choose to share their personal tracker data may have different characteristics than those who do not, which could also skew the results.

  * ROCCC:
    * Reliability: The dataset provides minute-level output for physical activity, heart rate, and sleep monitoring, which can be considered reliable and consistent.
    * Objectivity: It is unclear whether the data is free from bias or personal opinions, which could affect its objectivity.
    * Currency: The data was collected between 03.12.2016-05.12.2016, which may limit its relevance to current contexts.
    * Coverage: The data covers physical activity, heart rate, and sleep monitoring, but may not cover all aspects relevant to the problem being analyzed.
    * Comprehensiveness: The dataset includes personal tracker data from 30 eligible Fitbit users, which may not be comprehensive enough to draw generalizable conclusions.

* How are you addressing licensing, privacy, security, and accessibility?
  * Licensing: The data set is licensed under CC0, which means that it is in the public domain and can be used, modified, and distributed without any restrictions.

  * Privacy: The data set includes personal tracker data from Fitbit users, and therefore it is important to protect the privacy of the individuals who provided the data. It is not clear from the information provided how the wellness technology company in the case study is addressing privacy concerns. However, it is important for any company that handles personal data to have robust privacy policies and procedures in place to protect the privacy of its users.

  * Security: The wellness technology company in the case study is collecting personal data from users, including information on physical activity, heart rate, and sleep monitoring. Therefore, it is important for the company to have strong security measures in place to protect the data from unauthorized access or use.

  * Accessibility: The information provided does not give any indication of how the wellness technology company is addressing accessibility concerns. However, it is important for companies to ensure that their products and services are accessible to people with disabilities, and to comply with relevant accessibility standards and guidelines.

* How did you verify the data’s integrity? 
  * It means ensuring that the data is accurate, complete, and reliable.
  * Check the data source: By Verification that the data comes from a reliable and trustworthy source. In the case of the Fitbit data set mentioned in your earlier question, the data was collected through a survey distributed via Amazon Mechanical Turk.
  * Cross-check the data: It is not possible to Verify the accuracy of the data by cross-checking it against other sources of data or by conducting independent research on the same topic.
  * Check for errors and inconsistencies: It will be done during the proccess stage! :xD

* How does it help you answer your question? N.A

* Are there any problems with the data? N.A


<br>

#### Key tasks

* Download data and store it appropriately. 
  * Data is stored locally on a hard drive.

* Identify how it’s organized.
<br>

Reading the File:
```{r}
test <- read.csv("Fitabase/heartrate.csv")

```

Examine the structure of the data: 
```{r}
str(test)

```

Getting summary statistics for each variable:
```{r}
summary(test)
```
A sample of the data and also preview all the column names:
```{r}
head(test)
```


* Sort and filter the data.
  * It typically means to organize and subset the data to make it easier to work with and analyze. In the case of the 18 files of daily, hourly, and minute logs, I did sort the data by date or time to organize it chronologically. This would make it easier to track changes and patterns over time. 
  * Filtering the data means selecting a subset of the data based on specific criteria. For example, you could filter the data to focus on a particular date range or specific types of physical activities.
  * Finally to sort and filter the data in multiple files, I would need to combine the data into a single dataset and then use software tools like Excel, R, or Python to sort and filter the data. 



* Determine the credibility of the data.
  * We discussed about it earlier.
  
  
  
<br>
<br>
<br>

### 3. Process

<br>
<br>

#### Guiding questions 
* What tools are you choosing and why? 
  * I choosed to work with R, Because R is well-suited for this case study because it can handle large datasets, perform complex statistical analysis, and generate visualizations that can aid in understanding the data. Additionally, it has numerous packages and libraries that are specifically designed for machine learning and predictive modeling, which can be useful for identifying trends and making informed decisions about the company's products and services.

* Have you ensured your data’s integrity? 
  * Check Key Tasks.
* What steps have you taken to ensure that your data is clean? 
  * Check Key Tasks.
* How can you verify that your data is clean and ready to analyze? 
* Have you documented your cleaning process so you can review and share those results?

#### Key tasks
* Check the data for errors.
* Choose your tools. 
* Transform the data so you can work with it effectively. 

<br>
Loading Selective Data to answer the business question: (It was not easy to select these, I explored these data several times. Until I reached this conclusion to use only them. ) 

Loading Libraries
```{r}
# Installing and loading required packages
#install.packages("purrr")
#install.packages("corrplot")
#install.packages("GGally")
#install.packages("shiny")
#install.packages("plotly")
```


```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(lubridate)
library(GGally)
library(purrr)
library(janitor)
library(snakecase)
library(hms)
library(shiny)
library(plotly)
```


```{r}
dActivity    <- read.csv("Fitabase/daily/dailyActivity.csv")
dSleep         <- read.csv("Fitabase/daily/sleepDay.csv")

weightLogInfo    <- read.csv("Fitabase/weightLogInfo.csv")
heartRate    <- read.csv("Fitabase/heartrate.csv")

hCalories    <- read.csv("Fitabase/hourly/hourlyCalories.csv")
hIntensities    <- read.csv("Fitabase/hourly/hourlyIntensities.csv")
hSteps    <- read.csv("Fitabase/hourly/hourlySteps.csv")


mMETs    <- read.csv("Fitabase/minute/minuteMETsNarrow.csv")
mCalories    <- read.csv("Fitabase/minute/minuteCaloriesNarrow.csv")
mSteps    <- read.csv("Fitabase/minute/minuteStepsNarrow.csv")
mIntensities    <- read.csv("Fitabase/minute/minuteIntensitiesNarrow.csv")


```

A sample of the data and also preview all the column names:

```{r}
str(hCalories)
```




* Document the cleaning process.




Cleansing data is a most important part of a data analyst job, for analyzing the data, we need to merge daily tables, hour tables, and minute tables to gether to create three merged data frame for daily, hourly, minute data.    


Rename the date column in data frames (Preparing for anazlyzing):

```{r}
names(dActivity)[names(dActivity) == "ActivityDate"] <- "Time"
names(dSleep)[names(dSleep) == "SleepDay"] <- "Time"

names(hCalories)[names(hCalories) == "ActivityHour"] <- "Time"
names(hIntensities)[names(hIntensities) == "ActivityHour"] <- "Time"
names(hSteps)[names(hSteps) == "ActivityHour"] <- "Time"

names(mMETs)[names(mMETs) == "ActivityMinute"] <- "Time"
names(mCalories)[names(mCalories) == "ActivityMinute"] <- "Time"
names(mSteps)[names(mSteps) == "ActivityMinute"] <- "Time"
names(mIntensities)[names(mIntensities) == "ActivityMinute"] <- "Time"






#names(weightLogInfo)[names(weightLogInfo) == "Date"] <- "Date"
```





We start with clean_names(), that is a function from the janitor package in R, which can be used to clean column names in a data frame by converting them to a consistent and clean format. We start our cleansing process by this function. 

```{r}
# Clean column names and capitalize first letter of each word


# Create a list of data frames

assign("dActivity", dActivity)
assign("dSleep", dSleep)
assign("heartRate", heartRate)
assign("hCalories", hCalories)
assign("hIntensities", hIntensities)
assign("hSteps", hSteps)
assign("mMETs", mMETs)
assign("mCalories", mCalories)
assign("mSteps", mSteps)
assign("mIntensities", mIntensities)



# function to clean and rename column names
clean_rename_cols <- function(df) {
  df <- df %>% 
    clean_names() %>% 
    rename_all(~ to_any_case(., case = "upper_camel"))
  
  return(df)
}

name_of_dataframes = c("dActivity", "dSleep", "heartRate", "hCalories", "hIntensities", "hSteps", "mMETs", "mCalories", "mSteps", "mIntensities")
# loop through the list of data frames
for (df_name in name_of_dataframes) {
  # get the original data frame
  df <- get(df_name)
  
  # clean and rename the columns
  df_clean <- clean_rename_cols(df)
  
  # overwrite the original data frame with the cleaned version
  assign(df_name, df_clean)
}

```

```{r}
head(dActivity)
```

Check for missing values: Use functions like is.na() or complete.cases() to check for missing values in the data. We can then decide whether to remove the rows or fill in the missing values.

```{r}
# Create a function to check missing values of a data frame
check_missing_values <- function(df) {
  missing_count <- sum(is.na(df))
  if(missing_count > 0) {
    print(paste("Missing values in ", df_name), ":", missing_count)
  } else {
    print(paste("No missing values in ", df_name))
  }
}

# Apply the function to each data frames in the list

for (df_name in name_of_dataframes) {
  # get the original data frame
  df <- get(df_name)
  check_missing_values(df)
}
```
Next step is to check the dataframes for duplicated rows (accross all the columns)
```{r}
# identify duplicated rows across all columns
# Finding the duplicated rows in each data frames:

duplicated_rows <- function(df, df_name) {
  
  # check for duplicated rows across all columns
  dup_rows <- df[duplicated(df) | duplicated(df, fromLast = TRUE), ]
  
  if (nrow(dup_rows) > 0) {
    message("Duplicates found in ", df_name)
    print(dup_rows)
  } else {
    message("No duplicates found in ", df_name)
  }
}

for (df_name in name_of_dataframes) {
  # get the original data frame
  df <- get(df_name)
  duplicated_rows(df, df_name)
}


```

To remove duplicate rows based on all columns in data frame df, we can use:
```{r}
dSleep <- distinct(dSleep)
duplicated_rows(dSleep, "dSleep")
str(dSleep)
```

Address Inconsistencies: Apply tolower() to all character columns in a data frame to standardize the data. I commented this line, because there is no Char columns in dataset.  
```{r}
# apply tolower() to all character columns
#df_clean[, sapply(df_clean, is.character)] <- lapply(df_clean[, sapply(df_clean, #is.character)], tolower)
```


Address Inconsistencies: to find the rows that have inconsistent values in a particular column, and check if selected column is Date, or Date-Time:

(For daily df, because they dont contain time values.)
```{r}


# This function check for inconsistencies in Time column for Daily df
check_date_consistency <- function(df, date_col, df_name) {
  # extract date column
  df_date <- df[[date_col]]

  # check for NA values in date column and remove them
  na_count <- sum(is.na(as.Date(df_date, format = "%m/%d/%Y")))
  df <- df[!is.na(as.Date(df_date, format = "%m/%d/%Y")), ]
  
  if (na_count > 0) {
    message(paste("Warning: There are", na_count, "NA values in the date column in", df_name))
    message(paste("Removed", na_count, "rows with NA values"))
  }
  
  # check for inconsistencies in date format in date column
  date_inconsistent <- !all(grepl("^\\d{2}/\\d{2}/\\d{4}$", df_date))
  if (date_inconsistent) {
    message(paste("Warning: There are inconsistencies in the date format in", df_name))
    # fix the inconsistencies by converting the date column to the correct format
    df[[date_col]] <- as.Date(df_date, format = "%m/%d/%Y")
    message("Fixed the inconsistencies in the date format")
  }
  else {
    message(paste("Passed: Column contains consistent date format in", df_name))
  }
  
  # change date format to "%Y-%m-%d"
  df[[date_col]] <- format(as.Date(df[[date_col]], format = "%m/%d/%Y"), "%Y-%m-%d")
  
  # print final message
  message(paste(df_name, "is consistent in the date column"))
  
  # return the modified data frame
  return(df)
}


for (df_name in c("dActivity", "dSleep")) {
  # get the original data frame
  df <- get(df_name)
  df <- check_date_consistency(df, "Time", df_name)
  
  # update the data frame that df_name is pointed to
  assign(df_name, df)
  message(paste("Updated", df_name))
}

```

For Hourly, Minute, and HR df: 
```{r}
# This function check for inconsistencies in Time column for df
# To check for inconsistencies in a date-time column, you can use the lubridate package in R, which provides various functions to work with date-time data.

# This function check for inconsistencies in Time column for Daily df
check_date_consistency <- function(df, date_col, df_name) {
  # extract date column
  df_date <- df[[date_col]]
  
  # check for NA values in date column and remove them
  na_count <- sum(is.na(as.POSIXct(df_date, format = "%m/%d/%Y %I:%M:%S %p")))
  df <- df[!is.na(as.POSIXct(df_date, format = "%m/%d/%Y %I:%M:%S %p")), ]
  
  if (na_count > 0) {
    message(paste("Warning: There are", na_count, "NA values in the date column in", df_name))
    message(paste("Removed", na_count, "rows with NA values"))
  }
  
  # check for inconsistencies in date format in date column
  date_inconsistent <- !all(grepl("^\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2}:\\d{2} [AP]M$", df_date))
  if (date_inconsistent) {
    message(paste("Warning: There are inconsistencies in the date format in", df_name))
    # fix the inconsistencies by converting the date column to the correct format
    df[[date_col]] <- format(as.POSIXct(df_date, format = "%m/%d/%Y %I:%M:%S %p"), "%Y-%m-%d %H:%M:%S")
    #df$date_col <- as.POSIXct(df$date_col, format = "%Y-%m-%d %H:%M:%S")
    message("Fixed the inconsistencies in the date format")
  }
  else {
    message(paste("Passed: Column contains consistent date format in", df_name))
  }
  
  # print final message
  message(paste(df_name, "is consistent in the date column"))
  
  # return the modified data frame
  return(df)
}

for (df_name in c("heartRate", "hCalories", "hIntensities", "hSteps", "mMETs", "mCalories", "mSteps", "mIntensities")) {
  # get the original data frame
  df <- get(df_name)
  df <- check_date_consistency(df, "Time", df_name)
  
  # update the data frame that df_name is pointed to
  assign(df_name, df)
  message(paste("Updated", df_name))
}
```


Convert the Time column from char type to datetime format. (Preparing for analyzing):

```{r}
dActivity$Time <- as.Date(dActivity$Time, format="%Y-%m-%d")
dSleep$Time <- as.Date(dSleep$Time, format="%Y-%m-%d")

for (df_name in c("heartRate", "hCalories", "hIntensities", "hSteps", "mMETs", "mCalories", "mSteps", "mIntensities")) {
  # get the original data frame
  df <- get(df_name)
  df$Time <- as.POSIXct(df$Time, format="%Y-%m-%d %H:%M:%S")
  
  # update the data frame that df_name is pointed to
  assign(df_name, df)
  message(paste("Updated", df_name))
}


#weightLogInfo$Date <- as.Date(weightLogInfo$Date, format="%m/%d/%Y")

```


Finding Inconsistencies: to find if there is any inconsistent values in a particular column, and check columns type - they must be integer/double:
```{r}
for (df_name in name_of_dataframes) {
  # get the original data frame
  df <- get(df_name)
  message(paste("Structure of ", df_name))
  str(df)
  
  
}
```




Finally, merging daily tables together, then hourly tables, and minute ones. 
```{r}

# Create a list of the data frames you want to merge:
daily <- list(dActivity,dSleep)
hourly <- list(hCalories, hIntensities, hSteps)
minute <- list(mMETs, mCalories, mSteps, mIntensities)

```

For merging the data frames based on the common column 'Id', and 'Date', First we need to Create a list of the data frames we want to merge:



Then Use the inner_join function to merge the data frames, specifying the common columns to join on:
```{r}

df_daily <- daily %>% 
  reduce(inner_join, by = c("Id", "Time"))

df_hourly <- hourly %>% 
  reduce(inner_join, by = c("Id", "Time"))

df_minute <- minute %>% 
  reduce(inner_join, by = c("Id", "Time"))
```


I decided to use info from weightLogInfo df. Because it is not a complete dataset based on Date and ID, (I noticed that there are only 8 unique Id, and weight recorded only in a few random days.),  so I only selected BMI, WeightKg and Id. However, because there were several different BMI and Weight info per Id, I used their mean to have a unique value per Id. I think we can use this dataframe later in analyze stage.

```{r}
# Compute means of WeightKg and BMI by Id
weightLogInfo_mod <- weightLogInfo %>%
  group_by(Id) %>%
  summarize(mean_weight = mean(WeightKg, na.rm = TRUE),
            mean_bmi = mean(BMI, na.rm = TRUE)) %>%
  select(Id, mean_weight, mean_bmi)

# Merge with df_merged

#df_merged_with_weight <- merge(df_merged, weightLogInfo_mod, by = "Id")
```



<br>
<br>
<br>

### 3. Analyze

<br>
<br>

#### Guiding questions 

<br>
* How should you organize your data to perform analysis on it? 
* Has your data been properly formatted? 
* What surprises did you discover in the data?
* What trends or relationships did you find in the data?
* How will these insights help answer your business questions?

<br>
<br>

#### Key tasks
* Aggregate your data so it’s useful and accessible.
* Organize and format your data.
* Perform calculations.
* Identify trends and relationships.

Lets check the cleaned data, It is time to explore data statistically, we may find interesting info xD
```{r}
# Copy of data, and rename it 
head(df_daily, 5)
head(df_hourly, 5)
head(df_minute, 5)
```

Adding new Column, and assign combination of "Id" + integer seq to each ID. It can help to analyze the data easier at later stages.
```{r}
dActivity$newId <- paste0(letters[as.numeric(factor(dActivity$Id))])
df_daily$newId <- paste0(letters[as.numeric(factor(df_daily$Id))])
df_hourly$newId <- paste0(letters[as.numeric(factor(df_hourly$Id))])
df_minute$newId <- paste0(letters[as.numeric(factor(df_minute$Id))])
heartRate$newId <- paste0(letters[as.numeric(factor(heartRate$Id))])

```


```{r}
# check the first 5 rows of a random df
head(df_hourly, 5)
```



View the last 5 rows of a random data frame
```{r}
tail(df_hourly, 5)
```

An overview of the structure of data

```{r}
str(df_minute)
```
Summary Statistics: it provides information such as minimum, 1st quartile, median, mean, 3rd quartile, and maximum values, as well as the number of missing values (if any).

```{r}
summary(heartRate)
```

The table() function in R is used to create a contingency table of counts or proportions. It takes one or more variables as input and returns a table showing the frequency of occurrence of each combination of values in the variables.
```{r}
table(heartRate$newId)

# 

```

```{r}
table(df_hourly$newId)


```

```{r}
table(df_minute$newId)

```

```{r}
table(dActivity$newId)

```


In this particular case, the output indicates that there are 27 unique values in the Id column, which are:

a: 31 times
b: 31 times
c: 30 times
d: 31 times
e: 31 times
f: 31 times
g: 31 times
h: 31 times
i: 18 times
j: 31 times
k: 20 times
l: 30 times
m: 31 times
n: 4 times
NA: 203 times
o: 31 times
p: 31 times
q: 31 times
r: 31 times
s: 31 times
t: 31 times
u: 30 times
v: 28 times
w: 29 times
x: 26 times
y: 31 times
z: 26 times

This table is helpful for identifying the frequency of each unique value in a categorical column, which can provide insights into the distribution of the data. It means that for some IDs there are few or no observations in the dataset. For example, the ID "n" has only 4 observations, while the ID "NA" has 203 observations. This could be due to different reasons, such as data collection problems, participants dropping out of the study, or simply a smaller sample size for those IDs.


Aggregate the data:

```{r}
modified_summary_of_data <- function(data, df_name) {
  
  # Aggregate the data
  df_summary <- data %>%
    group_by(newId) %>%
    summarize_all(list(mean = mean, sd = sd))
  
  # count the number of unique Id in df
  unique_id_count <- data %>% 
    distinct(newId) %>% 
    nrow()
  
  message(paste("Unique ID count of", df_name, " = ", unique_id_count))
  #cat(paste(rep("*", 27), collapse = " "))
  
  return(list(df_summary = df_summary, unique_id_count = unique_id_count))
}

assign("df_daily", df_daily)
assign("df_hourly", df_hourly)
assign("df_minute", df_minute)

for (df_name in c("dActivity", "heartRate", "df_daily", "df_hourly", "df_minute")) {
  # get the original data frame
  df <- get(df_name)
  result <- modified_summary_of_data(df, df_name)
  #result$df_summary  # to access df_summary
  #result$unique_id_count  # to access unique_id_count
  print(head(result$df_summary, 5))
  #message(paste("Updated", df_name))
}

```


Visualize the data:
I like this part of analyzing. :P



```{r}
ggplot(dActivity, aes(x = newId, y = Calories)) +
  geom_point() +
  ggtitle("Calories vs. Fitbit Users") +
  xlab("New ID") +
  ylab("Calories") 
 
```


```{r}
dActivity %>% 
  group_by(newId) %>% 
  summarize(mean_Calories = mean(Calories)) %>% 
  ggplot(aes(x = newId, y = mean_Calories, fill = newId)) +
  geom_bar(stat = "identity") +
  ggtitle("Calories vs. Fitbit Users") +
  xlab("Fitbit Users") +
  ylab("Calories") 


```

```{r}
# Save the ggplot separately
ggsave("caloriesPlot.png", dpi = 300)
```


I think Correlation matrix can help a lot to discover insights in a tables with a lot of parameters. In other hand it seems to me that there is no strong correlation between Id (Participants) and other parameters. Because Id is just a random number assigned to each participants. It kinda their names. So for better analyzing once we can drop non-numeric columns. 
```{r}
# Select only numeric columns
data_no_id <- dActivity %>% select(-c(Id, newId, TrackerDistance))
data_numeric <- data_no_id[, sapply(data_no_id, is.numeric)]

# Calculate the correlation matrix
cor_matrix <- cor(data_numeric)

colnames(dActivity)
```


Visualize correlation matrix

```{r}

corrplot(cor_matrix,number.cex = 0.7, tl.cex = 0.8, method = "circle")

```

To get better insight, setting a threshold can help us. 
```{r}
cor_mat_filtered <- cor_matrix[abs(cor_matrix) >= 0.5]


```

This means that it selects both positive and negative correlation coefficients whose magnitude is 0.5 or greater. So it includes both correlation coefficients greater than +0.5 and those less than -0.5. (meaningful correlation coefficient ranges)

Check the new heatmap:
```{r}

# Set the threshold for correlation coefficients to be displayed
cor_mat_filtered <- cor_matrix
cor_mat_filtered[abs(cor_mat_filtered) < 0.5] <- 0

# Generate the correlation plot
corrplot(cor_mat_filtered, method = "number", type = "lower", addCoef.col = 2, number.cex = 0.5, tl.cex = 0.5)





```




As heatmap represents,  There is a clear correlation between these pair parameters: 
(Calories vs TotalDistance), (Calories vs VeryActiveMinutes), (SedentaryMinutes vs TotalTimeInBed), (SedentaryMinutes vs TotalMinutesAsleep), (FairlyActiveMinutes vs ModeratelyActiveDistance), (LightActiveDistance vs LightlyActiveMinutes), (VeryActiveDistance vs VeryActiveMinutes)

Lets Explore more about them:

```{r}
# This code is used to add a smoothed line to a ggplot using the gam method and specify the color of the line as purple. The formula argument specifies the formula to use in the smoothing, where y is the dependent variable and x is the independent variable transformed with a smooth function s(). The se argument is set to FALSE to remove the shaded confidence interval around the line.
#
dActivity %>% 
  ggplot(aes(x= TotalDistance, y= Calories, color=TotalDistance)) +
  geom_point() +
  geom_smooth(method="gam", formula = y ~s(x), se=FALSE, color="purple") +
  labs(title="Calories Vs Total Distance") +
    xlab("Total Distance") +
  ylab("Calories")
  theme_grey()
```



```{r}
dActivity %>% 
  ggplot(aes(x= VeryActiveMinutes, y= Calories, color= VeryActiveMinutes)) +
  geom_point() +
  geom_smooth(method="gam", formula = y ~s(x), se=FALSE, color="purple") +
  labs(title="Calories Vs VeryActiveMinutes") +
    xlab("VeryActiveMinutes") +
  ylab("Calories")
  theme_grey()
```


```{r}
df_daily %>% 
  ggplot(aes(x= SedentaryMinutes, y= TotalMinutesAsleep, color= TotalMinutesAsleep)) +
  geom_point() +
  geom_smooth(method="gam", formula = y ~s(x), se=FALSE, color="purple") +
  labs(title="Total Minutes Asleep Vs Sedentary Minutes") +
    xlab("Sedentary Minutes") +
  ylab("Total Minutes Asleep")
  theme_grey()
```


```{r}
df_daily %>% 
  ggplot(aes(x= SedentaryMinutes, y= TotalTimeInBed, color= TotalTimeInBed)) +
  geom_point() +
  geom_smooth(method="gam", formula = y ~s(x), se=FALSE, color="purple") +
  labs(title="Total Minutes in Bed Vs Sedentary Minutes") +
    xlab("Sedentary Minutes") +
  ylab("Total Minutes in Bed")
  theme_grey()
```


```{r}
dActivity %>% 
  ggplot(aes(x= FairlyActiveMinutes, y= ModeratelyActiveDistance, color= -ModeratelyActiveDistance)) +
  geom_point() +
  geom_smooth(method="gam", formula = y ~s(x), se=FALSE, color="orange") +
  labs(title="Fairly Active Minutes Vs Moderately Active Distance") +
    xlab("Fairly Active Minutes") +
  ylab("Moderately Active Distance")
  theme_grey()
  

```




```{r}

# subset the data for the relevant columns
sub_data <- dActivity[, c("VeryActiveMinutes", "TotalDistance", "Calories")]


ggpairs(sub_data, title="Very Active Minutes vs Moderately Active Distance vs Calories")+theme_gray() 
   
  


```




```{r}
ggpairs(sub_data,
upper = list(continuous = "density", combo = "box_no_facet"),
lower = list(continuous = "points", combo = "dot_no_facet"),
title="Very Active Minutes vs Moderately Active Distance vs Calories"

)
```

Those graphs were beautiful, But they could not answer the bussiness questions. It was like a warm up. xD
Ok, Lets find ways to answer our questions. 

1. The number of times users opened the app,
2. The features they used most frequently,
3. The time of day they were most active on the app.

* The number of times users opened the app,
  * To determine the number of times users opened the app, you would need to check if the Fitband data frames contain any information about app usage. If there is no such data available, then this question cannot be answered.

* The features they used most frequently,
  * To determine the features that users used most frequently, we can start by identifying the columns in each data frame that correspond to different features.To calculate summary statistics for each feature in the dActivity data frame:
  
```{r}
# Group by each feature and calculate the mean and median
dActivity_summary <- dActivity %>%
  summarize_at(vars(TotalSteps:SedentaryActiveDistance, VeryActiveMinutes:SedentaryMinutes, Calories), list(mean = mean, median = median))

# Print the summary table
dActivity_summary
```
 demonstrates how we can count the number of times users opened the app and determine the time of day they were most active on the app:
 
```{r}
# Count the number of unique Ids (i.e., the number of users) in the data frame
num_users <- dActivity %>%
  distinct(Id) %>%
  nrow()

# Count the number of unique dates in the data frame
num_dates <- dActivity %>%
  distinct(date(Time)) %>%
  nrow()

# Calculate the number of times users opened the app (i.e., the number of rows in the data frame)
num_app_opens <- nrow(dActivity)

# Function to get the most active day of the month and the most active month for each unique user
get_most_active_time <- function(df) {
  # Create a new column for the day of the month
  df <- df %>% mutate(day = day(Time))
  
  # Calculate the most active day of the month and the most active month for each unique user
  most_active_time <- df %>%
    mutate(month = month(Time, label = TRUE)) %>%
    group_by(newId) %>%
    summarize(most_active_day = which.max(table(day)),
              most_active_month = which.max(table(month))) %>%
    ungroup()
  
  return(most_active_time)
}

x = get_most_active_time(dActivity)

# Print the results
cat(paste0("Number of users: ", num_users, "\n"))
cat(paste0("Number of dates: ", num_dates, "\n"))
cat(paste0("Number of app opens: ", num_app_opens, "\n"))
print(x)
```

DActivity Data Frame:

```{r}
# Create a new data frame with the day of the month and count of app usage for each user
day_counts <- dActivity %>%
  mutate(day = day(Time)) %>%
  group_by(newId, day) %>%
  summarize(count = n())

# Create a stacked bar chart using ggplot
ggplot(day_counts, aes(x = factor(day), y = count, fill = newId)) +
  geom_bar(stat = "identity") +
  labs(x = "Day of the Month", y = "Count of App Usage", fill = "User ID") +
  ggtitle("Daily App Usage by Day of the Month") +
  theme_bw()

```
 
```{r}
# Create a new data frame with the day of the month and count of app usage for each user
day_counts <- dActivity %>%
  mutate(month = month(Time)) %>%
  group_by(newId, month) %>%
  summarize(count = n())

# Create a stacked bar chart using ggplot
ggplot(day_counts, aes(x = factor(month), y = count, fill = newId)) +
  geom_bar(stat = "identity") +
  labs(x = "Months", y = "Count of App Usage", fill = "User ID") +
  ggtitle("Daily App Usage by the Month") +
  theme_bw()
```


Hourly Data Frame:

```{r}
# Create a new data frame with the day of the month and count of app usage for each user
day_counts <- df_hourly %>%
  mutate(day = day(Time)) %>%
  group_by(newId, day) %>%
  summarize(count = n())

# Create a stacked bar chart using ggplot
ggplot(day_counts, aes(x = factor(day), y = count, fill = newId)) +
  geom_bar(stat = "identity") +
  labs(x = "Hour of the Day", y = "Count of App Usage", fill = "User ID") +
  ggtitle("Hourly App Usage of the Day") +
  theme_bw()

```

```{r}
# Create a new data frame with the day of the month and count of app usage for each user
day_counts <- df_hourly %>%
  mutate(hour = hour(Time)) %>%
  group_by(newId, hour) %>%
  summarize(count = n())

# Create a stacked bar chart using ggplot
ggplot(day_counts, aes(x = factor(hour), y = count, fill = newId)) +
  geom_bar(stat = "identity") +
  labs(x = "Hour of the Day", y = "Count of App Usage", fill = "User ID") +
  ggtitle("Hourly App Usage of the Day") +
  theme_bw()
```



Minute Data Frame:

```{r}
# Create a new data frame with the day of the month and count of app usage for each user
day_counts <- df_minute %>%
  mutate(day = day(Time)) %>%
  group_by(newId, day) %>%
  summarize(count = n())

# Create a stacked bar chart using ggplot
ggplot(day_counts, aes(x = factor(day), y = count, fill = newId)) +
  geom_bar(stat = "identity") +
  labs(x = "Hour of the Day", y = "Count of App Usage", fill = "User ID") +
  ggtitle("Hourly App Usage of the Day") +
  theme_bw()

```

```{r}
# Create a new data frame with the day of the month and count of app usage for each user
day_counts <- df_minute %>%
  mutate(hour = hour(Time)) %>%
  group_by(newId, hour) %>%
  summarize(count = n())

# Create a stacked bar chart using ggplot
ggplot(day_counts, aes(x = factor(hour), y = count, fill = newId)) +
  geom_bar(stat = "identity") +
  labs(x = "Hour of the Day", y = "Count of App Usage", fill = "User ID") +
  ggtitle("Hourly App Usage of the Day") +
  theme_bw()
```



```{r}
# Create a new data frame with the day of the month and count of app usage for each user
day_counts <- df_daily %>%
  mutate(day = day(Time)) %>%
  group_by(newId, day) %>%
  summarize(count = n())

# Create a stacked bar chart using ggplot
ggplot(day_counts, aes(x = factor(day), y = count, fill = newId)) +
  geom_bar(stat = "identity") +
  labs(x = "Day of the Month", y = "Count of App Usage", fill = "User ID") +
  ggtitle("Daily App Usage by Day of the Month") +
  theme_bw()
```
 Heart Rate:
 
```{r}
# Create a new data frame with the day of the month and count of app usage for each user
day_counts <- heartRate %>%
  mutate(hour = hour(Time)) %>%
  group_by(newId, hour) %>%
  summarize(count = n())

# Create a stacked bar chart using ggplot
ggplot(day_counts, aes(x = factor(hour), y = count, fill = newId)) +
  geom_bar(stat = "identity") +
  labs(x = "Hour of the Day", y = "Count of App Usage", fill = "User ID") +
  ggtitle("Hourly App Usage of the Day") +
  theme_bw()

```

```{r}
# Create a new data frame with the day of the month and count of app usage for each user
day_counts <- heartRate %>%
  mutate(day = day(Time)) %>%
  group_by(newId, day) %>%
  summarize(count = n())

# Create a stacked bar chart using ggplot
ggplot(day_counts, aes(x = factor(day), y = count, fill = newId)) +
  geom_bar(stat = "identity") +
  labs(x = "Hour of the Day", y = "Count of App Usage", fill = "User ID") +
  ggtitle("Hourly App Usage of the Day") +
  theme_bw()
```
```{r}
# Create a new data frame with the day of the month and count of app usage for each user
day_counts <- heartRate %>%
  mutate(month = month(Time)) %>%
  group_by(newId, month) %>%
  summarize(count = n())

# Create a stacked bar chart using ggplot
ggplot(day_counts, aes(x = factor(month), y = count, fill = newId)) +
  geom_bar(stat = "identity") +
  labs(x = "Hour of the Day", y = "Count of App Usage", fill = "User ID") +
  ggtitle("Hourly App Usage of the Day") +
  theme_bw()
```

 
 
  
3. The time of day they were most active on the app.

Finding Total Duration time that each specific user recorded data in each day, for 3 different main data frames. 
Clearly it will give us enough information about which features were popular, and users liked to get info about those. and which one were less popular among all users. Three main data frames that I selected are : Heart Rate, Minute recorded data (MeTs, Calories, Steps, Activity), and daily Sleep duration data frame.
```{r}

heartRate <- heartRate %>% rename(HeartRate = Value)

Sleep_duration <- dSleep %>% 
  rename(Date = Time) %>% 
  select(Id, Date, TotalMinutesAsleep)



# Calculate duration for each Id on each day for df_minute
MApp_duration <- df_minute %>%
  group_by(Id, Date = as.Date(Time)) %>%
  summarise(ActivityDuration = difftime(max(Time), min(Time), units = "secs"))
# Convert DurationTime column to h:m:s format
MApp_duration$ActivityDuration <- as_hms(MApp_duration$ActivityDuration)


# Calculate duration for each Id on each day for heartRate df
HRApp_duration <- heartRate %>%
  group_by(Id, Date = as.Date(Time)) %>%
  summarise(HRcheckDuration = difftime(max(Time), min(Time), units = "secs"))


# Convert DurationTime column to h:m:s format
HRApp_duration$HRcheckDuration <- as_hms(HRApp_duration$HRcheckDuration)

# Left join the three data frames by "Id" and "Date" columns
merged_data <- left_join(MApp_duration, HRApp_duration, by = c("Id", "Date")) %>%
               left_join(Sleep_duration, by = c("Id", "Date"))

write.csv(df, "merged_data.csv", row.names = FALSE)

```



Visualizing and representing finding with a dashboard. Yaaaaay: :))))

Creating a Dashboard!
```{r}

#merged_data <- read.csv("merged_data.csv")

# Define UI
ui <- fluidPage(
  # Add title
  titlePanel("My Dashboard"),
  
  # Add sidebar with filters
  sidebarLayout(
    sidebarPanel(
      selectInput("day", "Select Day:", choices = unique(as.Date(merged_data$Date))),
      selectInput("id", "Select ID:", choices = unique(merged_data$Id))
    ),
    
    # Add main panel with plots
    mainPanel(
      # Arrange plotlyOutputs horizontally using fluidRow and column functions
      fluidRow(
        column(width = 4, plotlyOutput("pie_activity")),
        column(width = 4, plotlyOutput("pie_hrcheck")),
        column(width = 4, plotlyOutput("pie_sleep"))
      )
    )
  )
)

# Define server
server <- function(input, output) {
  # Create reactive data frame based on selected filters
  selected_row <- reactive({
    filter(merged_data, Date == input$day, Id == input$id)
  })
  
  # Create pie charts for each column
  output$pie_activity <- renderPlotly({
    AC <- as.numeric(selected_row()$ActivityDuration)
    plot_ly(values = c(AC, 86400 - AC), type = "pie", hole = 0.6) %>%
      layout(title = "Activity App Usage")
  })
  
  output$pie_hrcheck <- renderPlotly({
    HR <- as.numeric(selected_row()$HRcheckDuration)
    plot_ly(values = c(HR, 86400 - HR), type = "pie", hole = 0.6) %>%
      layout(title = "Heart Rate App Usage")
  })
  
  output$pie_sleep <- renderPlotly({
    TotalMinutesAsleep <- selected_row()$TotalMinutesAsleep
    plot_ly(values = c(TotalMinutesAsleep, 1440 - TotalMinutesAsleep), type = "pie", hole = 0.6) %>%
      layout(title = "Sleep App Usage")
  })
}


```


Time to See:
```{r}
# Run the app

shinyApp(ui = ui, server = server)

```


The following code creates three pie charts to summarize the daily usage of three different apps (Activity, Heart Rate, and Sleep), based on the mean duration of usage per day and idle time. The charts are then combined into a subplot with two rows and two columns. 
```{r}
# Calculate total row numbers
total_rows <- nrow(merged_data)
# Calculate means
mean_AC <- as.numeric(sum(merged_data$ActivityDuration, na.rm=TRUE) / total_rows)
mean_HR <- as.numeric(sum(merged_data$HRcheckDuration, na.rm=TRUE) / total_rows)
mean_TA <- sum(merged_data$TotalMinutesAsleep, na.rm=TRUE) / total_rows

# Calculate total row numbers
total_rows <- nrow(merged_data)

# Color defining
colors <- c('rgb(39, 194, 80)', 'rgb(225, 242, 31)')



# Create pie chart for Activity App Usage
fig1 <- plot_ly (values = c(mean_AC, 86400 - mean_AC), 
        textposition = 'inside',
        textinfo = 'label+percent',
        insidetextfont = list(color = '#FFFFFF'),
        hoverinfo = 'text',
        marker = list(colors = colors, line = list(color = '#FFFFFF', width = 1)),
        type = "pie",
        hole = 0.5,
        labels = c("Activity App Usage Per day", "Idle Time"),
        showlegend = FALSE,
        domain = list(x = c(0, 0.33), y = c(0, 1))) %>%
        layout(title = "Summary Activity App Usage")

# Create pie chart for Heart Rate App Usage
fig2 <- plot_ly (values = c(mean_HR, 86400 - mean_HR), 
        textposition = 'inside',
        textinfo = 'label+percent',
        insidetextfont = list(color = '#FFFFFF'),
        hoverinfo = 'text',
        marker = list(colors = colors, line = list(color = '#FFFFFF', width = 1)),
        type = "pie",
        hole = 0.5,
        labels = c("Heart Rate App Usage Per day", "Idle Time"),
        showlegend = FALSE,
        domain = list(x = c(0.33, 0.66), y = c(0, 1))) %>%
        layout(title = "Summary Heart Rate App Usage")

# Create pie chart for Activity App
fig3 <- plot_ly (values = c(mean_TA, 1440 - mean_TA), 
        textposition = 'inside',
        textinfo = 'label+percent',
        insidetextfont = list(color = '#FFFFFF'),
        hoverinfo = 'text',
        marker = list(colors = colors, line = list(color = '#FFFFFF', width = 1)),
        type = "pie",
        hole = 0.5,
        labels = c("Sleep App Usage Per day", "Idle Time"),
        showlegend = FALSE,
        domain = list(x = c(0.66, 1), y = c(0, 1))) %>%
        layout(title = "Summary Sleep App Usage")

# Hide warning messages
options(warn=-1)

# Display the pie charts
subplot(fig1,fig2, fig3, nrows = 2, widths = c(0.5, 0.5), heights = c(0.5, 0.5))

```

Comparing same data by Bar Chart!

```{r}



# Color defining
colors <- c('rgb(20, 224, 78)', 'rgb(53, 138, 230)', 'rgb(247, 47, 47)')

# Create bar chart for App Usage
plot_ly(x = c("Activity App Usage Per day", "Heart Rate App Usage Per day", "Sleep App Usage Per day"), 
        y = c(mean_AC / 3600, mean_HR / 3600, mean_TA / 60), 
        type = "bar", 
        marker = list(color = colors)) %>%
  layout(title = "Summary App Usage", 
         xaxis = list(title = "Apps"), 
         yaxis = list(title = "Average Usage Hours Per Day"))





```

It is clear that Activity features (Activity App) is more popular than other ones. 
Activity features are:
"MeTs", "Calories", "Steps", "Intensity"

<br>
<br>
<br>

### 3. Share

<br>
<br>

#### Guiding questions 
<br>

* Were you able to answer the business questions?
  * Yes, the findings answer the business question of which features are most popular among users.
  
* What story does your data tell?
  * The data shows that users prefer devices that support daily activity features such as MeTs, Calories, Steps, and Intensity, and use them for an average of 23.5 hours per day. Heart rate monitoring devices are used regularly, but less than activity ones, with an average usage of 7.5 hours daily. Sleep monitoring devices have the lowest usage, with an average of 3 hours daily.
  
* How do your findings relate to your original question? 
  * The findings help the wellness technology company understand which features are most popular among users and which devices are used more often. This information can guide the development and marketing of new products.
  
* Who is your audience? What is the best way to communicate with them? 
  * The audience for this information would likely be the product development and marketing teams within the wellness technology company. The best way to communicate with them is through a presentation or report highlighting the key findings and providing actionable insights for product development and marketing strategies.

* Can data visualization help you share your findings? 
  * Yes, data visualization can be a useful tool for sharing the findings of this analysis. Creating charts and graphs can help visually illustrate users' usage patterns and preferences.

* Is your presentation accessible to your audience?
  * The accessibility of the presentation will depend on the format and delivery method chosen. If the presentation is shared online, it should be formatted in a way that is accessible to all users, including those with disabilities. I included providing alternative text for images, using accessible colours, and ensuring the content is navigable using a keyboard.

<br>

#### Key tasks 
<br>

* Determine the best way to share your findings. 
  * Done
* Create effective data visualizations. 
  * Done
* Present your findings.
  * Done
* Ensure your work is accessible.
  * Done



<br>
<br>
<br>

### 3. Act

<br>
<br>

#### Guiding questions 
<br>

* What is your final conclusion based on your analysis? 
* How could your team and business apply your insights? 
* What next steps would you or your stakeholders take based on your findings? 
* Is there additional data you could use to expand on your findings?


```{r}




# Color defining
colors <- c('rgb(20, 224, 78)', 'rgb(53, 138, 230)', 'rgb(247, 47, 47)')

# Create bar chart for App Usage
plot_ly(x = c("Activity App Usage Per day", "Heart Rate App Usage Per day", "Sleep App Usage Per day"), 
        y = c(mean_AC / 3600, mean_HR / 3600, mean_TA / 60), 
        type = "bar", 
        marker = list(color = colors)) %>%
  layout(title = "Summary App Usage", 
         xaxis = list(title = "Apps"), 
         yaxis = list(title = "Average Usage Hours Per Day"))





```


* Final Conclusion: Activity features (Activity App) are the most popular among users, with Fitband users preferring to use this app more than others in their daily usage. The devices that support Daily Activity features are used almost daily, and the average usage is around 23.5 hours daily. Devices that check Heart Rate are also popular, but less than Activity ones, with the average hour of usage around 7.5 hours per day. Sleep monitoring devices are the least popular, with only some users interested in using them, and the average usage is around 3 hours per day.

* Applying insights: Based on these insights, the wellness technology company could focus on improving the Activity App and making it more engaging for Fitband users. They could also consider developing new devices that support Daily Activity features, as they are the most popular among users. The company could also use these insights to optimize their marketing efforts and promote the most popular features to potential customers.

* Next Steps: The team could conduct further research on the specific features that users find most engaging within the Activity App and explore ways to improve those features. The team could also analyze the data to identify the demographic segments most interested in using each device and feature and tailor their marketing efforts accordingly.

* Additional data: The team could gather additional user feedback and preferences data to expand on their findings. They could also analyze user behaviour across different geographic regions to identify regional trends and tailor their marketing efforts accordingly. Additionally, they could collect data on the usage of other wellness-related features, such as nutrition and mental health, to gain a more comprehensive understanding of user preferences and behaviour.
